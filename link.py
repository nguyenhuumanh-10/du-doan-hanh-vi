import streamlit as st
import joblib
import numpy as np
import cv2
import mediapipe as mp
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
from collections import deque, Counter

# ============================
# C·∫§U H√åNH & H√ÄM TI·ªÜN √çCH
# ============================
WINDOW_SIZE = 30
SMOOTH_WINDOW = 5
CLASSES = ["left", "right", "nod", "yawn", "blink", "normal"]
EPS = 1e-6
EYE_LEFT_IDX = np.array([33, 159, 145, 133, 153, 144])
EYE_RIGHT_IDX = np.array([362, 386, 374, 263, 380, 385])
MOUTH_IDX = np.array([61, 291, 0, 17, 78, 308])

def eye_aspect_ratio(landmarks, left=True):
    idx = EYE_LEFT_IDX if left else EYE_RIGHT_IDX
    pts = landmarks[idx, :2]
    A = np.linalg.norm(pts[1] - pts[5])
    B = np.linalg.norm(pts[2] - pts[4])
    C = np.linalg.norm(pts[0] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def mouth_aspect_ratio(landmarks):
    pts = landmarks[MOUTH_IDX, :2]
    A = np.linalg.norm(pts[0] - pts[1])
    B = np.linalg.norm(pts[4] - pts[5])
    C = np.linalg.norm(pts[2] - pts[3])
    return (A + B) / (2.0 * (C + EPS))

def head_pose_yaw_pitch_roll(landmarks):
    left_eye, right_eye, nose, chin = landmarks[33][:2], landmarks[263][:2], landmarks[1][:2], landmarks[152][:2]
    dx, dy = right_eye - left_eye
    roll = np.degrees(np.arctan2(dy, dx + EPS))
    interocular = np.linalg.norm(right_eye - left_eye) + EPS
    eyes_center = (left_eye + right_eye) / 2.0
    yaw = np.degrees(np.arctan2((nose[0] - eyes_center[0]), interocular))
    baseline = chin - eyes_center
    pitch = np.degrees(np.arctan2((nose[1] - eyes_center[1]), (np.linalg.norm(baseline) + EPS)))
    return yaw, pitch, roll

# ============================
# GIAO DI·ªÜN STREAMLIT
# ============================
st.set_page_config(layout="wide")
st.title("üëÅÔ∏è H·ªá Th·ªëng D·ª± ƒêo√°n H√†nh Vi L√°i Xe")

try:
    # ============================
    # 1. T·∫£i m√¥ h√¨nh v√† scaler
    # ============================
    @st.cache_resource
    def load_model_and_scaler():
        model_data = joblib.load("svm_hinge_model.pkl")
        scaler = joblib.load("scaler.pkl")
        return model_data, scaler

    model_data, scaler = load_model_and_scaler()
    W, b = model_data["W"], model_data["b"]

    # ============================
    # 2. Thi·∫øt l·∫≠p MediaPipe
    # ============================
    @st.cache_resource
    def load_face_mesh():
        return mp.solutions.face_mesh.FaceMesh(
            max_num_faces=1, refine_landmarks=True,
            min_detection_confidence=0.5, min_tracking_confidence=0.5)
    
    face_mesh = load_face_mesh()

    # ============================
    # 3. L·ªõp x·ª≠ l√Ω video (T√çCH H·ª¢P LOGIC N√ÇNG CAO)
    # ============================
    class VideoProcessor(VideoTransformerBase):
        def __init__(self):
            self.buffer = deque(maxlen=WINDOW_SIZE)
            self.label_buffer = deque(maxlen=SMOOTH_WINDOW)
            self.last_label = "normal"
            self.status = "ƒêang t√¨m khu√¥n m·∫∑t..."

        def transform(self, frame):
            img = frame.to_ndarray(format="bgr24")
            img = cv2.flip(img, 1)
            h, w, _ = img.shape
            
            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            results = face_mesh.process(rgb)

            if results.multi_face_landmarks:
                self.status = "ƒêang ph√¢n t√≠ch..."
                face = results.multi_face_landmarks[0]
                landmarks = np.array([[p.x * w, p.y * h, p.z * w] for p in face.landmark])

                # T√≠nh c√°c ƒë·∫∑c tr∆∞ng c∆° b·∫£n
                ear_l = eye_aspect_ratio(landmarks, True)
                ear_r = eye_aspect_ratio(landmarks, False)
                mar = mouth_aspect_ratio(landmarks)
                yaw, pitch, roll = head_pose_yaw_pitch_roll(landmarks)
                nose, chin = landmarks[1], landmarks[152]
                angle_pitch_extra = np.degrees(np.arctan2(chin[1]-nose[1], chin[2]-nose[2]+EPS))
                forehead_y = np.mean(landmarks[[10,338,297,332,284],1])
                cheek_dist = np.linalg.norm(landmarks[50]-landmarks[280])
                
                # Th√™m v√†o buffer
                self.buffer.append([ear_l, ear_r, mar, yaw, pitch, roll, angle_pitch_extra, forehead_y, cheek_dist])

                if len(self.buffer) == WINDOW_SIZE:
                    self.status = "ƒêang d·ª± ƒëo√°n..."
                    # T√≠nh c√°c ƒë·∫∑c tr∆∞ng th·ªëng k√™ tr√™n c·ª≠a s·ªï
                    window = np.array(self.buffer)
                    mean_feats, std_feats = window.mean(axis=0), window.std(axis=0)
                    yaw_diff, pitch_diff, roll_diff = np.mean(np.abs(np.diff(window[:,3]))), np.mean(np.abs(np.diff(window[:,4]))), np.mean(np.abs(np.diff(window[:,5])))
                    mar_series, ear_series = window[:,2], (window[:,0]+window[:,1])/2.0
                    mar_max, mar_mean = np.max(mar_series), np.mean(mar_series)
                    mar_ear_ratio = mar_mean / (np.mean(ear_series)+EPS)
                    yaw_pitch_ratio = np.mean(np.abs(window[:,3])) / (np.mean(np.abs(window[:,4]))+EPS)
                    
                    # Gh√©p n·ªëi t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng l·∫°i
                    feats = np.concatenate([mean_feats, std_feats, [yaw_diff, pitch_diff, roll_diff, mar_max, mar_ear_ratio, yaw_pitch_ratio]]).reshape(1,-1)
                    
                    # Chu·∫©n h√≥a v√† d·ª± ƒëo√°n
                    X_input = scaler.transform(feats)
                    scores = X_input @ W + b
                    cls_id = int(np.argmax(scores))
                    label = CLASSES[cls_id]

                    # L√†m m∆∞·ª£t k·∫øt qu·∫£
                    self.label_buffer.append(label)
                    smooth_label = Counter(self.label_buffer).most_common(1)[0][0]
                    self.last_label = smooth_label
            else:
                self.status = "Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t"
                self.buffer.clear() # X√≥a buffer n·∫øu m·∫•t m·∫∑t
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            cv2.putText(img, self.last_label.upper(), (30,60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 3)
            cv2.putText(img, f"Status: {self.status}", (30, h-30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
            
            return img

    # ============================
    # 4. Kh·ªüi ch·∫°y webcam (S·ª¨ D·ª§NG TURN SERVER ƒê·ªÇ V∆Ø·ª¢T T∆Ø·ªúNG L·ª¨A)
    # ============================
    st.info("üí°Cho ph√©p tr√¨nh duy·ªát truy c·∫≠p camera v√† nh√¨n th·∫≥ng v√†o webcam.")

    # C·∫•u h√¨nh n√†y bao g·ªìm c·∫£ STUN v√† m·ªôt m√°y ch·ªß TURN c√¥ng c·ªông (openrelay)
    # ƒë·ªÉ c·ªë g·∫Øng v∆∞·ª£t qua c√°c t∆∞·ªùng l·ª≠a nghi√™m ng·∫∑t.
    webrtc_streamer(
        key="webcam",
        video_processor_factory=VideoProcessor,
        media_stream_constraints={"video": True, "audio": False},
        rtc_configuration={
            "iceServers": [
                {"urls": ["stun:stun.l.google.com:19302"]},
                {"urls": ["stun:stun1.l.google.com:19302"]},
                {
                    "urls": [
                        "turn:openrelay.metered.ca:80",
                        "turn:openrelay.metered.ca:443",
                    ],
                    "username": "openrelayproject",
                    "credential": "openrelayproject",
                },
            ]
        }
    )

# ============================
# 5. KH·ªêI B·∫ÆT L·ªñI
# ============================
except Exception as e:
    st.error("L·ªñI NGHI√äM TR·ªåNG ƒê√É X·∫¢Y RA")
    st.error("Vui l√≤ng ki·ªÉm tra l·∫°i c√°c file m√¥ h√¨nh v√† ƒë·∫£m b·∫£o c√°c th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë·∫ßy ƒë·ªß.")
    st.exception(e)
